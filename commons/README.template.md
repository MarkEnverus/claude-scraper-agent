# {SOURCE} {DATA_TYPE} Scraper

> **Generated by:** Claude Scraper Agent v{SCRAPER_VERSION}
> **Infrastructure Version:** {INFRASTRUCTURE_VERSION}
> **Generated Date:** {GENERATED_DATE}
> **Collection Method:** {COLLECTION_METHOD}

## Overview

This scraper collects **{DATA_TYPE}** data from **{SOURCE}** using {COLLECTION_METHOD}.

**Update Frequency:** {UPDATE_FREQUENCY}
**Historical Support:** {HISTORICAL_SUPPORT}
**Authentication:** {AUTH_METHOD}

## Data Source Details

**Source:** {SOURCE}
**Data Type:** {DATA_TYPE}
**Format:** {DATA_FORMAT}

{COLLECTION_METHOD_DETAILS}

## Quick Start

### 1. Prerequisites

- Python 3.9+
- UV package manager
- Redis server (for hash registry)
- AWS credentials configured (for S3 storage)

### 2. Environment Variables

```bash
# Required
export {SOURCE_UPPER}_API_KEY="your-api-key-here"  # {AUTH_DESCRIPTION}
export REDIS_HOST="localhost"
export REDIS_PORT="6379"
export AWS_PROFILE="your-aws-profile"  # or AWS_ACCESS_KEY_ID/AWS_SECRET_ACCESS_KEY

# Optional
export {SOURCE_UPPER}_BASE_URL="{BASE_URL}"  # Override default base URL
export LOG_LEVEL="INFO"  # DEBUG, INFO, WARNING, ERROR
```

### 3. Installation

```bash
# Install dependencies
uv pip install -r requirements.txt

# Or use the monorepo environment
cd /path/to/sourcing-project
uv sync
```

### 4. Run the Scraper

#### Collect Latest Data
```bash
python {SCRAPER_FILENAME}
```

#### Collect Historical Data
```bash
python {SCRAPER_FILENAME} --start-date 2025-01-01 --end-date 2025-01-10
```

#### Dry Run (No S3 Upload)
```bash
python {SCRAPER_FILENAME} --dry-run
```

## Usage Examples

### Basic Usage
```bash
# Collect data for today
python {SCRAPER_FILENAME}

# Collect data for specific date range
python {SCRAPER_FILENAME} --start-date 2025-01-01 --end-date 2025-01-31

# Collect with specific hours (if applicable)
python {SCRAPER_FILENAME} --start-date 2025-01-01 --end-date 2025-01-01 --hours 0,6,12,18
```

### Advanced Options
```bash
# Dry run (test without uploading to S3)
python {SCRAPER_FILENAME} --dry-run --start-date 2025-01-01 --end-date 2025-01-02

# Force re-download (skip hash check)
python {SCRAPER_FILENAME} --force --start-date 2025-01-01

# Verbose logging
LOG_LEVEL=DEBUG python {SCRAPER_FILENAME} --start-date 2025-01-01
```

## Command Line Interface

```
Usage: {SCRAPER_FILENAME} [OPTIONS]

Options:
  --start-date TEXT  Start date (YYYY-MM-DD) [default: today]
  --end-date TEXT    End date (YYYY-MM-DD) [default: tomorrow]
  --dry-run          Test run without uploading to S3
  --force            Force re-download (skip hash registry check)
  --help             Show this message and exit
```

## Data Storage

### S3 Storage Pattern

Data is stored in S3 with the following structure:

```
s3://{S3_BUCKET}/{S3_PREFIX}/{SOURCE}/{DATA_TYPE}/
  └── YYYY/
      └── MM/
          └── DD/
              └── {FILENAME_PATTERN}
```

**Example:**
```
s3://your-bucket/raw-data/{SOURCE_LOWER}/{DATA_TYPE_LOWER}/
  └── 2025/
      └── 01/
          └── 15/
              └── {DATA_TYPE_LOWER}_20250115_14.{FILE_EXTENSION}
```

### Redis Hash Registry

The scraper uses Redis to track downloaded files and prevent duplicate downloads:

**Key Pattern:** `hash:{SOURCE_LOWER}_{DATA_TYPE_LOWER}:<hash>`

**Hash Calculation:**
- Based on file content (MD5)
- Prevents re-downloading identical data
- Use `--force` flag to bypass

## Data Format

**Input Format:** {DATA_FORMAT}
**Output Format:** {OUTPUT_FORMAT}

{DATA_FORMAT_DETAILS}

## Testing

### Run Unit Tests
```bash
# Run all tests for this scraper
pytest sourcing/scraping/{SOURCE_LOWER}/tests/test_{SCRAPER_FILENAME} -v

# Run with coverage
pytest sourcing/scraping/{SOURCE_LOWER}/tests/test_{SCRAPER_FILENAME} -v --cov

# Run specific test
pytest sourcing/scraping/{SOURCE_LOWER}/tests/test_{SCRAPER_FILENAME}::test_generate_candidates -v
```

### Test Coverage
- ✅ Candidate generation
- ✅ Data collection
- ✅ Content validation
- ✅ Error handling
- ✅ Authentication
- ✅ Date range handling

### Fixtures
Test fixtures are located in: `sourcing/scraping/{SOURCE_LOWER}/tests/fixtures/`

## Code Quality

This scraper has been validated with:
- ✅ **mypy**: Type checking (0 errors)
- ✅ **ruff**: Style and linting (0 issues)
- ✅ **pytest**: All tests passing
- ✅ **UV standards**: pyproject.toml configured

## Architecture

### Class Structure

```python
class {CLASS_NAME}(BaseCollector):
    """Collector for {SOURCE} {DATA_TYPE} data."""

    def __init__(self, api_key: str, **kwargs):
        # Initialize with authentication

    def generate_candidates(self, start_date, end_date) -> List[DownloadCandidate]:
        # Generate list of data to collect

    def collect_content(self, candidate: DownloadCandidate) -> bytes:
        # Collect data for a single candidate

    def validate_content(self, content: bytes) -> bool:
        # Validate collected data (optional)
```

### Collection Framework

This scraper extends `BaseCollector` from the collection framework, which provides:
- Hash registry integration (Redis)
- S3 upload functionality
- Logging and error handling
- Retry logic
- Kafka notifications (optional)

## Error Handling

The scraper handles common errors:
- **Authentication Errors**: Invalid API key, expired credentials
- **Rate Limiting**: Automatic retry with exponential backoff
- **Network Errors**: Connection timeouts, DNS failures
- **Data Validation**: Invalid JSON, missing required fields
- **S3 Upload Errors**: Permissions, bucket not found

**Error Logs:** Check logs for detailed error messages and stack traces.

## Monitoring

### Kafka Notifications

The scraper sends notifications to Kafka for monitoring:

**Topic:** `data-collection-events`

**Event Types:**
- `collection.started` - Scraper started
- `collection.completed` - Scraper completed successfully
- `collection.failed` - Scraper failed with error
- `file.uploaded` - Individual file uploaded to S3

**Payload Example:**
```json
{
  "event_type": "collection.completed",
  "source": "{SOURCE_LOWER}",
  "data_type": "{DATA_TYPE_LOWER}",
  "timestamp": "2025-01-15T14:30:00Z",
  "files_collected": 24,
  "files_uploaded": 22,
  "files_skipped": 2,
  "duration_seconds": 45.2
}
```

### Logging

**Log Levels:**
- `DEBUG`: Detailed execution trace
- `INFO`: Normal operations (default)
- `WARNING`: Unexpected but handled situations
- `ERROR`: Failures requiring attention

**Log Format:** Structured JSON for easy parsing

## Configuration

### Authentication

{AUTH_CONFIGURATION_DETAILS}

### API Rate Limits

{RATE_LIMIT_DETAILS}

### Customization

To customize the scraper behavior:

1. **Modify candidate generation:** Edit `generate_candidates()` method
2. **Change API parameters:** Update `collection_params` in candidates
3. **Add validation logic:** Override `validate_content()` method
4. **Adjust retry logic:** Modify BaseCollector kwargs

## Troubleshooting

### Common Issues

**Issue: "Authentication failed"**
```bash
# Check API key is set
echo $<SOURCE_UPPER>_API_KEY

# Test API key manually
curl -H "X-API-Key: $<SOURCE_UPPER>_API_KEY" {BASE_URL}/test
```

**Issue: "Redis connection refused"**
```bash
# Check Redis is running
redis-cli ping

# Start Redis
redis-server

# Or use Docker
docker run -d -p 6379:6379 redis:latest
```

**Issue: "S3 upload failed"**
```bash
# Check AWS credentials
aws sts get-caller-identity

# Test S3 access
aws s3 ls s3://your-bucket/
```

**Issue: "No data collected"**
```bash
# Run in dry-run mode with debug logging
LOG_LEVEL=DEBUG python {SCRAPER_FILENAME} --dry-run --start-date 2025-01-01

# Check if data source has data for requested dates
# Verify API endpoints are correct
```

## Maintenance

### Updating the Scraper

If the data source API changes, use the scraper updater:

```bash
/scraper-dev:update-scraper
```

### Version History

- **v{SCRAPER_VERSION}** ({GENERATED_DATE}): Initial generation
  - Generated with Claude Scraper Agent
  - Infrastructure version {INFRASTRUCTURE_VERSION}
  - QA validated (mypy, ruff, pytest)

## Support

### Resources

- **Collection Framework:** `sourcing/scraping/commons/collection_framework.py`
- **Hash Registry:** `sourcing/scraping/commons/hash_registry.py`
- **Logging:** `sourcing/common/logging_json.py`

### Getting Help

For issues or questions:
1. Check the troubleshooting section above
2. Review error logs for detailed messages
3. Consult the collection framework documentation
4. Contact the data engineering team

## License

Proprietary - Internal use only
