---
# Scraper Configuration Example
# Place this file at: sourcing/scraping/{dataSource}/{dataSet}/.scraper-dev.md
# Remove .example from filename and customize values

# ============================================
# REQUIRED FIELDS
# ============================================

data_source: NYISO
data_type: load_forecast
collection_method: HTTP/REST API
data_format: JSON
update_frequency: hourly
historical_support: yes
authentication: API Key

# ============================================
# HTTP/REST API CONFIGURATION
# (Only needed if collection_method is "HTTP/REST API")
# ============================================

api_base_url: https://api.nyiso.com/v1
api_endpoint: /load/hourly
api_query_params: date,hour,zone
api_rate_limit: 60 requests/minute
api_pagination: none

# ============================================
# WEBSITE PARSING CONFIGURATION
# (Only needed if collection_method is "Website Parsing")
# ============================================

website_url_pattern: https://example.com/data/{date}
website_link_selector: a.download-link
website_requires_js: no
website_file_pattern: load_*.csv

# ============================================
# FTP/SFTP CONFIGURATION
# (Only needed if collection_method is "FTP/SFTP file download")
# ============================================

ftp_host: ftp.example.com
ftp_port: 21
ftp_directory: /data/load
ftp_file_pattern: load_*.csv
ftp_mode: passive

# ============================================
# EMAIL ATTACHMENTS CONFIGURATION
# (Only needed if collection_method is "Email attachments")
# ============================================

email_server: imap.gmail.com
email_port: 993
email_mailbox: INBOX
email_subject_filter: Daily Load Report.*
email_sender_filter: .*@nyiso\.com
email_attachment_pattern: .*\.csv

---

# Additional Notes (Optional)

This configuration file is for the NYISO hourly load forecast scraper.

## Usage

1. Copy this file to your dataset directory:
   ```
   cp .scraper-dev.example.md sourcing/scraping/nyiso/load_forecast/.scraper-dev.md
   ```

2. Edit the values to match your data source

3. Run `/create-scraper` in Claude Code

4. The agent will:
   - Find your config file automatically
   - Use configured values without asking
   - Only prompt for values not in the config

## Config File Location

Place config files at:
```
sourcing/scraping/{dataSource}/{dataSet}/.scraper-dev.md
```

Examples:
- `sourcing/scraping/nyiso/load_forecast/.scraper-dev.md`
- `sourcing/scraping/pjm/price_actual/.scraper-dev.md`
- `sourcing/scraping/caiso/wind_generation/.scraper-dev.md`

## Multi-Project Support

If you have multiple config files (mono-repo), the agent will:
1. Scan and find all `.scraper-dev.md` files
2. Ask which dataset you want to configure
3. Use the selected config file

## Field Reference

### collection_method Options
- `HTTP/REST API` - For REST API endpoints
- `Website Parsing (scraping HTML pages)` - For web scraping
- `FTP/SFTP file download` - For FTP/SFTP servers
- `Email attachments` - For email-based data delivery

### data_format Options
- `CSV`, `JSON`, `XML`, `HTML`, `PDF`, `Excel`

### update_frequency Options
- `real-time`, `every 5 minutes`, `hourly`, `daily`, `weekly`

### historical_support Options
- `yes` - Source supports date range queries
- `no` - Source only provides latest data

### authentication Options
- `API Key`, `OAuth 2.0`, `Basic Auth`, `Certificate`, `None`
