# {{ source }} {{ data_type | title }} Data Collector

Automated data collector for {{ source }} {{ data_type }} data.

## Overview

This scraper collects {{ data_type }} data from {{ source }} using their {{ collection_method }} interface.

- **Data Source**: {{ source }}
- **API Base URL**: {{ api_base_url }}
- **Authentication**: {{ auth_method }}
- **Data Format**: {{ data_format | upper }}
- **Update Frequency**: {{ update_frequency }}
{% if historical_support %}
- **Historical Data**: Supported
{% else %}
- **Historical Data**: Not available
{% endif %}

## Features

- ✅ Automated data collection with retry logic
- ✅ Content deduplication using Redis hash registry
- ✅ S3 storage with date partitioning (YYYY/MM/DD)
- ✅ Kafka notifications for real-time data pipelines
- ✅ Comprehensive error handling and logging
- ✅ CLI interface with flexible configuration

## Data Endpoints

This scraper collects data from {{ endpoints | length }} endpoints:

{% for endpoint in endpoints %}
### {{ loop.index }}. {{ endpoint.display_name }}

- **Endpoint ID**: `{{ endpoint.name }}`
- **Path**: `{{ endpoint.path }}`
- **Method**: `{{ endpoint.method }}`
- **Description**: {{ endpoint.description }}
{% if endpoint.params %}
- **Parameters**: {{ endpoint.params | tojson }}
{% endif %}

{% endfor %}

## Installation

### Prerequisites

- Python 3.11+
- Redis (for hash registry)
- AWS credentials (for S3 storage)
- Kafka cluster (optional, for notifications)

### Dependencies

```bash
pip install -r requirements.txt
```

Required packages:
- `click` - CLI interface
- `redis` - Hash registry
- `boto3` - S3 storage
- `kafka-python` - Kafka notifications
- `requests` - HTTP client
{% if collection_method == "WEBSITE_PARSER" %}
- `beautifulsoup4` - HTML parsing
- `selenium` - JavaScript rendering
{% endif %}

## Configuration

### Environment Variables

Set these environment variables or pass as CLI options:

| Variable | Required | Description |
|----------|----------|-------------|
{% if auth_required %}
| `{{ auth_env_var }}` | Yes | {{ auth_method }} authentication key |
{% endif %}
| `REDIS_HOST` | No | Redis host (default: localhost) |
| `REDIS_PORT` | No | Redis port (default: 6379) |
| `S3_BUCKET` | Yes | S3 bucket for data storage |
| `KAFKA_BOOTSTRAP_SERVERS` | No | Kafka servers (comma-separated) |
| `KAFKA_TOPIC` | No | Kafka topic (default: {{ dgroup }}) |

### Authentication

{% if auth_required %}
This scraper requires authentication via {{ auth_method }}.

{% if auth_method == "API_KEY" %}
**API Key Setup:**

1. Register for API access at {{ source }}
2. Obtain your API key
3. Set environment variable:
   ```bash
   export {{ auth_env_var }}="your-api-key-here"
   ```

The API key is sent in the `{{ auth_header_name }}` header.
{% elif auth_method == "BEARER_TOKEN" %}
**Bearer Token Setup:**

1. Register for API access at {{ source }}
2. Obtain your bearer token
3. Set environment variable:
   ```bash
   export {{ auth_env_var }}="your-bearer-token-here"
   ```

The token is sent in the `Authorization: Bearer <token>` header.
{% elif auth_method == "BASIC_AUTH" %}
**Basic Authentication Setup:**

1. Register for API access at {{ source }}
2. Obtain your username:password credentials
3. Set environment variable:
   ```bash
   export {{ auth_env_var }}="username:password"
   ```

Credentials are base64-encoded and sent in the `Authorization: Basic <encoded>` header.
{% endif %}

{% else %}
This scraper does not require authentication.
{% endif %}

## Usage

### Command Line

Basic usage:

```bash
python {{ filename }} \
    --start-date 2025-01-01 \
    --end-date 2025-01-31 \
{% if auth_required %}
    --api-key YOUR_API_KEY \
{% endif %}
    --s3-bucket your-bucket-name \
    --kafka-bootstrap-servers localhost:9092
```

### CLI Options

| Option | Description |
|--------|-------------|
| `--start-date` | Start date for data collection (YYYY-MM-DD) |
| `--end-date` | End date for data collection (YYYY-MM-DD) |
{% if auth_required %}
| `--api-key` | {{ auth_method }} authentication key |
{% endif %}
| `--redis-host` | Redis host (default: localhost) |
| `--redis-port` | Redis port (default: 6379) |
| `--s3-bucket` | S3 bucket for data storage (required) |
| `--kafka-bootstrap-servers` | Kafka servers (comma-separated) |
| `--kafka-topic` | Kafka topic for notifications |
| `--dgroup` | Data group identifier (default: {{ dgroup }}) |
| `--debug` | Enable debug logging |

### Examples

**Collect data for a single day:**

```bash
python {{ filename }} \
    --start-date 2025-12-19 \
    --end-date 2025-12-19 \
{% if auth_required %}
    --api-key ${{ auth_env_var }} \
{% endif %}
    --s3-bucket production-data
```

**Collect historical data with debug logging:**

```bash
python {{ filename }} \
    --start-date 2025-01-01 \
    --end-date 2025-12-31 \
{% if auth_required %}
    --api-key ${{ auth_env_var }} \
{% endif %}
    --s3-bucket production-data \
    --kafka-bootstrap-servers kafka1:9092,kafka2:9092 \
    --debug
```

**Using environment variables:**

```bash
export {{ auth_env_var }}="your-api-key"
export S3_BUCKET="production-data"
export KAFKA_BOOTSTRAP_SERVERS="localhost:9092"

python {{ filename }} \
    --start-date 2025-01-01 \
    --end-date 2025-01-31
```

## Data Output

### S3 Storage Structure

Data is stored in S3 with date-based partitioning:

```
s3://your-bucket/
└── {{ dgroup }}/
    └── YYYY/
        └── MM/
            └── DD/
                └── {{ source_snake }}_{{ data_type_snake }}_ENDPOINT_YYYYMMDD_HHMMSS_VERSION.{{ data_format }}
```

Example:
```
s3://production-data/{{ dgroup }}/2025/01/15/{{ source_snake }}_{{ data_type_snake }}_{{ endpoints[0].name }}_20250115_143022_001.{{ data_format }}
```

### Data Format

Data is stored in {{ data_format | upper }} format.

{% if data_format == "json" %}
**JSON Structure:**
```json
{
    "data": [
        {
            "timestamp": "2025-01-15T14:30:00Z",
            "value": 123.45
        }
    ],
    "meta": {
        "source": "{{ source }}",
        "endpoint": "{{ endpoints[0].name }}",
        "collected_at": "2025-01-15T14:30:22Z"
    }
}
```
{% elif data_format == "csv" %}
**CSV Structure:**
```csv
timestamp,value,unit
2025-01-15T14:30:00Z,123.45,MW
2025-01-15T14:45:00Z,234.56,MW
```
{% elif data_format == "xml" %}
**XML Structure:**
```xml
<?xml version="1.0" encoding="UTF-8"?>
<data source="{{ source }}">
    <record>
        <timestamp>2025-01-15T14:30:00Z</timestamp>
        <value>123.45</value>
    </record>
</data>
```
{% endif %}

### Kafka Notifications

When Kafka is configured, the scraper sends notifications for each new file:

```json
{
    "event": "new_file",
    "dgroup": "{{ dgroup }}",
    "source": "{{ source_lower }}",
    "data_type": "{{ data_type_lower }}",
    "endpoint": "{{ endpoints[0].name }}",
    "date": "2025-01-15",
    "s3_path": "s3://bucket/{{ dgroup }}/2025/01/15/file.{{ data_format }}",
    "file_hash": "abc123...",
    "timestamp": "2025-01-15T14:30:22Z"
}
```

## Testing

### Run Unit Tests

```bash
pytest tests/{{ source_snake }}/test_{{ filename.replace('.py', '') }}.py -v
```

### Run Integration Tests

Integration tests require valid credentials:

```bash
export {{ auth_env_var }}="your-api-key"
pytest tests/{{ source_snake }}/test_{{ filename.replace('.py', '') }}.py -v -m integration
```

### Test Coverage

```bash
pytest tests/{{ source_snake }}/ --cov={{ source_snake }} --cov-report=html
```

## Infrastructure

This scraper uses the collection framework infrastructure:

- **BaseCollector**: Abstract base class for all collectors
- **HashRegistry**: Redis-based SHA256 content deduplication
- **S3Manager**: S3 upload with versioning and date partitioning
- **KafkaProducer**: Real-time notifications for data pipelines

See `infrastructure/` directory for implementation details.

## Monitoring

### Logging

The scraper uses structured JSON logging:

```python
{
    "timestamp": "2025-01-15T14:30:22Z",
    "level": "INFO",
    "message": "Collection complete",
    "extra": {
        "total_candidates": 31,
        "new_files": 28,
        "duplicates": 3,
        "errors": 0
    }
}
```

### Metrics

Key metrics tracked:
- Total candidates generated
- New files collected
- Duplicate files skipped
- Errors encountered
- Collection duration

## Troubleshooting

### Common Issues

**Authentication Errors:**
- Verify {{ auth_env_var }} is set correctly
- Check API key is valid and not expired
- Ensure API key has required permissions

**Connection Errors:**
- Verify network connectivity to {{ api_base_url }}
- Check firewall rules allow outbound HTTPS
- Retry with `--debug` flag for detailed logs

**Redis Errors:**
- Verify Redis is running: `redis-cli ping`
- Check Redis host/port configuration
- Ensure Redis has sufficient memory

**S3 Errors:**
- Verify AWS credentials are configured
- Check S3 bucket exists and is accessible
- Ensure IAM permissions for PutObject

## Maintenance

### Infrastructure Version

- **Version**: {{ infrastructure_version }}
- **Generated**: {{ generated_date }}
- **Generator**: {{ generator_agent }}

### Updating

To update to a new infrastructure version:

```bash
python -m claude_scraper.updater \
    --scraper {{ filename }} \
    --target-version X.Y.Z
```

## Support

For issues or questions:
1. Check logs with `--debug` flag
2. Review API documentation at {{ api_base_url }}
3. Contact data engineering team

## License

Proprietary - Internal use only
