"""Tests for ScraperUpdater."""

import pytest
from pathlib import Path
from unittest.mock import Mock, patch

from claude_scraper.fixers.updater import (
    ScraperUpdater,
    ScraperInfo,
    UpdaterResult,
    CURRENT_INFRASTRUCTURE_VERSION,
)


# ============================================================================
# FIXTURES
# ============================================================================

@pytest.fixture
def sample_scraper_v14():
    """Sample scraper content with v1.4.0."""
    return """\"\"\"MISO Energy Pricing Scraper - HTTP REST API.

Generated by: Claude Scraper Agent v1.4.0
Infrastructure version: 1.4.0

Data Source: MISO
Data Type: energy_pricing
Collection Method: HTTP REST API
\"\"\"

# GENERATOR_AGENT: http-collector-generator
# INFRASTRUCTURE_VERSION: 1.4.0
# LAST_UPDATED: 2024-01-01

import requests
"""


@pytest.fixture
def sample_scraper_v16():
    """Sample scraper content with v1.6.0 (current)."""
    return """\"\"\"SPP Price Scraper - FTP/SFTP.

Generated by: Claude Scraper Agent v1.6.0
Infrastructure version: 1.6.0

Data Source: SPP
Data Type: price_data
Collection Method: FTP/SFTP
\"\"\"

# GENERATOR_AGENT: ftp-collector-generator
# INFRASTRUCTURE_VERSION: 1.6.0
# LAST_UPDATED: 2024-01-15

import ftplib
"""


@pytest.fixture
def scraper_files(tmp_path, sample_scraper_v14, sample_scraper_v16):
    """Create sample scraper files."""
    scraper_root = tmp_path / "sourcing" / "scraping"
    scraper_root.mkdir(parents=True)

    # Outdated scraper
    outdated = scraper_root / "scraper_miso_energy_pricing_http.py"
    outdated.write_text(sample_scraper_v14)

    # Current scraper
    current = scraper_root / "scraper_spp_price_ftp.py"
    current.write_text(sample_scraper_v16)

    return {"root": scraper_root, "outdated": outdated, "current": current}


@pytest.fixture
def updater(scraper_files):
    """Create ScraperUpdater instance."""
    return ScraperUpdater(scraper_root=str(scraper_files["root"]))


# ============================================================================
# INITIALIZATION TESTS
# ============================================================================

def test_updater_initialization():
    """Test updater initializes with correct defaults."""
    updater = ScraperUpdater()
    assert updater.scraper_root == Path("sourcing/scraping")
    assert updater.infrastructure_root == Path("sourcing/scraping/commons")


def test_updater_initialization_custom_paths():
    """Test updater initializes with custom paths."""
    updater = ScraperUpdater(
        scraper_root="custom/scrapers",
        infrastructure_root="custom/infrastructure",
    )
    assert updater.scraper_root == Path("custom/scrapers")
    assert updater.infrastructure_root == Path("custom/infrastructure")


# ============================================================================
# SCAN SCRAPERS TESTS
# ============================================================================

def test_scan_scrapers_empty_directory(tmp_path):
    """Test scanning empty directory."""
    updater = ScraperUpdater(scraper_root=str(tmp_path))
    scrapers = updater.scan_scrapers()
    assert scrapers == []


def test_scan_scrapers_finds_all(updater, scraper_files):
    """Test scanning finds all scrapers."""
    scrapers = updater.scan_scrapers()
    assert len(scrapers) == 2


def test_scan_scrapers_identifies_outdated(updater, scraper_files):
    """Test scanning identifies outdated scrapers."""
    scrapers = updater.scan_scrapers()

    outdated = [s for s in scrapers if s.needs_update]
    current = [s for s in scrapers if not s.needs_update and not s.error]

    assert len(outdated) == 1
    assert len(current) == 1

    assert outdated[0].path.name == "scraper_miso_energy_pricing_http.py"
    assert outdated[0].current_version == "1.4.0"

    assert current[0].path.name == "scraper_spp_price_ftp.py"
    assert current[0].current_version == CURRENT_INFRASTRUCTURE_VERSION


def test_scan_scrapers_extracts_metadata(updater, scraper_files):
    """Test scanning extracts data source and type."""
    scrapers = updater.scan_scrapers()

    miso_scraper = next(s for s in scrapers if "miso" in s.path.name)
    assert miso_scraper.data_source == "MISO"
    assert miso_scraper.data_type == "energy_pricing"

    spp_scraper = next(s for s in scrapers if "spp" in s.path.name)
    assert spp_scraper.data_source == "SPP"
    assert spp_scraper.data_type == "price_data"


def test_scan_scrapers_handles_unreadable_file(tmp_path):
    """Test scanning handles unreadable files gracefully."""
    scraper_root = tmp_path / "sourcing" / "scraping"
    scraper_root.mkdir(parents=True)

    # Create a file we can't read (in practice this would be a permission issue)
    # For testing, we'll create a file and then mock the read to raise an error
    scraper_file = scraper_root / "scraper_broken.py"
    scraper_file.write_text("# test")

    updater = ScraperUpdater(scraper_root=str(scraper_root))

    with patch.object(Path, "read_text", side_effect=PermissionError("Access denied")):
        scrapers = updater.scan_scrapers()

    assert len(scrapers) == 1
    assert scrapers[0].error is not None
    assert "Access denied" in scrapers[0].error


# ============================================================================
# DETECT GENERATOR TESTS
# ============================================================================

def test_detect_generator_via_token(tmp_path):
    """Test detecting generator via explicit token (Method 1)."""
    scraper_file = tmp_path / "scraper_test.py"
    scraper_file.write_text("""
# GENERATOR_AGENT: http-collector-generator
# INFRASTRUCTURE_VERSION: 1.6.0

import requests
""")

    updater = ScraperUpdater()
    generator = updater.detect_generator_agent(scraper_file)

    assert generator == "http-collector-generator"


def test_detect_generator_via_collection_method(tmp_path):
    """Test detecting generator via collection method (Method 2)."""
    scraper_file = tmp_path / "scraper_test.py"
    scraper_file.write_text("""
\"\"\"Test Scraper.

Collection Method: FTP/SFTP
\"\"\"

import ftplib
""")

    updater = ScraperUpdater()
    generator = updater.detect_generator_agent(scraper_file)

    assert generator == "ftp-collector-generator"


def test_detect_generator_via_filename(tmp_path):
    """Test detecting generator via filename pattern (Method 3)."""
    updater = ScraperUpdater()

    # Test HTTP pattern
    http_file = tmp_path / "scraper_miso_http.py"
    http_file.write_text("import requests")
    assert updater.detect_generator_agent(http_file) == "http-collector-generator"

    # Test FTP pattern
    ftp_file = tmp_path / "scraper_spp_ftp.py"
    ftp_file.write_text("import ftplib")
    assert updater.detect_generator_agent(ftp_file) == "ftp-collector-generator"

    # Test website pattern
    website_file = tmp_path / "scraper_caiso_website.py"
    website_file.write_text("from bs4 import BeautifulSoup")
    assert updater.detect_generator_agent(website_file) == "website-parser-generator"

    # Test email pattern
    email_file = tmp_path / "scraper_ercot_email.py"
    email_file.write_text("import imaplib")
    assert updater.detect_generator_agent(email_file) == "email-collector-generator"


def test_detect_generator_via_code_patterns(tmp_path):
    """Test detecting generator via code analysis (Method 4)."""
    updater = ScraperUpdater()

    # Test HTTP pattern
    http_file = tmp_path / "scraper_test1.py"
    http_file.write_text("""
import requests

def fetch():
    response = requests.get("https://api.example.com")
    return response.json()
""")
    assert updater.detect_generator_agent(http_file) == "http-collector-generator"

    # Test FTP pattern
    ftp_file = tmp_path / "scraper_test2.py"
    ftp_file.write_text("""
import ftplib

ftp = ftplib.FTP()
""")
    assert updater.detect_generator_agent(ftp_file) == "ftp-collector-generator"

    # Test website pattern
    website_file = tmp_path / "scraper_test3.py"
    website_file.write_text("""
from bs4 import BeautifulSoup

soup = BeautifulSoup(html, 'html.parser')
""")
    assert updater.detect_generator_agent(website_file) == "website-parser-generator"


def test_detect_generator_fails_for_unknown(tmp_path):
    """Test detecting generator returns None for unknown scrapers."""
    scraper_file = tmp_path / "scraper_custom.py"
    scraper_file.write_text("""
# Custom scraper with no identifiable patterns
import custom_lib
""")

    updater = ScraperUpdater()
    generator = updater.detect_generator_agent(scraper_file)

    assert generator is None


# ============================================================================
# UPDATE SCRAPER TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_update_scraper_success(updater, scraper_files):
    """Test successful scraper update."""
    scrapers = updater.scan_scrapers()
    outdated = next(s for s in scrapers if s.needs_update)

    result = await updater.update_scraper(outdated, validate=False)

    assert isinstance(result, UpdaterResult)
    assert result.scraper_path == outdated.path
    assert result.old_version == "1.4.0"
    assert result.new_version == CURRENT_INFRASTRUCTURE_VERSION
    assert result.generator_agent == "http-collector-generator"
    assert result.error is None


@pytest.mark.asyncio
async def test_update_scraper_with_validation(updater, scraper_files):
    """Test scraper update with validation enabled."""
    scrapers = updater.scan_scrapers()
    outdated = next(s for s in scrapers if s.needs_update)

    result = await updater.update_scraper(outdated, validate=True)

    assert result.validation_passed is True


@pytest.mark.asyncio
async def test_update_scraper_updates_version_metadata(scraper_files):
    """Test update modifies version metadata."""
    updater = ScraperUpdater(scraper_root=str(scraper_files["root"]))

    scrapers = updater.scan_scrapers()
    outdated = next(s for s in scrapers if s.needs_update)

    # Get original content
    original_content = outdated.path.read_text()
    assert "# INFRASTRUCTURE_VERSION: 1.4.0" in original_content
    assert "# LAST_UPDATED: 2024-01-01" in original_content

    # Update scraper
    await updater.update_scraper(outdated, validate=False)

    # Verify metadata was updated
    updated_content = outdated.path.read_text()
    assert f"# INFRASTRUCTURE_VERSION: {CURRENT_INFRASTRUCTURE_VERSION}" in updated_content
    assert "# LAST_UPDATED: 2024-01-01" not in updated_content


@pytest.mark.asyncio
async def test_update_scraper_cannot_detect_generator(tmp_path):
    """Test update fails when cannot detect generator."""
    scraper_root = tmp_path / "sourcing" / "scraping"
    scraper_root.mkdir(parents=True)

    scraper_file = scraper_root / "scraper_unknown.py"
    scraper_file.write_text("""
# INFRASTRUCTURE_VERSION: 1.0.0
# Custom scraper
""")

    updater = ScraperUpdater(scraper_root=str(scraper_root))
    scrapers = updater.scan_scrapers()

    result = await updater.update_scraper(scrapers[0], validate=False)

    assert result.error is not None
    assert "Cannot detect generator" in result.error


@pytest.mark.asyncio
async def test_update_scraper_nonexistent_file(updater):
    """Test update fails for nonexistent file."""
    scraper_info = ScraperInfo(
        path=Path("nonexistent.py"),
        current_version="1.0.0",
        needs_update=True,
    )

    with pytest.raises(FileNotFoundError):
        await updater.update_scraper(scraper_info)


# ============================================================================
# REPORT GENERATION TESTS
# ============================================================================

def test_generate_scan_report(updater, scraper_files):
    """Test generating scan report."""
    scrapers = updater.scan_scrapers()
    report = updater.generate_scan_report(scrapers)

    assert "Scraper Version Report" in report
    assert f"Current Infrastructure Version: {CURRENT_INFRASTRUCTURE_VERSION}" in report
    assert "Outdated Scrapers (1):" in report
    assert "scraper_miso_energy_pricing_http.py" in report
    assert "Up-to-date Scrapers (1):" in report
    assert "scraper_spp_price_ftp.py" in report


def test_generate_scan_report_no_outdated(tmp_path, sample_scraper_v16):
    """Test scan report when all scrapers are up-to-date."""
    scraper_root = tmp_path / "sourcing" / "scraping"
    scraper_root.mkdir(parents=True)

    scraper_file = scraper_root / "scraper_current.py"
    scraper_file.write_text(sample_scraper_v16)

    updater = ScraperUpdater(scraper_root=str(scraper_root))
    scrapers = updater.scan_scrapers()
    report = updater.generate_scan_report(scrapers)

    assert "Outdated Scrapers" not in report
    assert "Up-to-date Scrapers (1):" in report


def test_generate_update_report_interactive():
    """Test generating update report in interactive mode."""
    results = [
        UpdaterResult(
            scraper_path=Path("scraper_miso.py"),
            old_version="1.4.0",
            new_version=CURRENT_INFRASTRUCTURE_VERSION,
            generator_agent="http-collector-generator",
            validation_passed=True,
        ),
        UpdaterResult(
            scraper_path=Path("scraper_spp.py"),
            old_version="1.3.0",
            new_version=CURRENT_INFRASTRUCTURE_VERSION,
            generator_agent="ftp-collector-generator",
            validation_passed=False,
        ),
        UpdaterResult(
            scraper_path=Path("scraper_ercot.py"),
            old_version="1.2.0",
            error="Generator failed",
        ),
    ]

    updater = ScraperUpdater()
    report = updater.generate_update_report(results, mode="interactive")

    assert "Scraper Update Complete!" in report
    assert "Successfully Updated & Validated (1):" in report
    assert "Updated but Needs Review (1):" in report
    assert "Failed to Update (1):" in report
    assert "Total processed: 3" in report


def test_generate_update_report_non_interactive():
    """Test generating update report in non-interactive mode."""
    results = [
        UpdaterResult(
            scraper_path=Path("scraper_miso.py"),
            old_version="1.4.0",
            new_version=CURRENT_INFRASTRUCTURE_VERSION,
            generator_agent="http-collector-generator",
            validation_passed=True,
        ),
    ]

    updater = ScraperUpdater()
    report = updater.generate_update_report(results, mode="non-interactive")

    assert "Non-Interactive Mode" in report


# ============================================================================
# INTEGRATION TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_full_update_workflow(tmp_path):
    """Test complete update workflow from scan to report."""
    # Setup
    scraper_root = tmp_path / "sourcing" / "scraping"
    scraper_root.mkdir(parents=True)

    # Create outdated scraper
    outdated_file = scraper_root / "scraper_miso_http.py"
    outdated_file.write_text("""
# GENERATOR_AGENT: http-collector-generator
# INFRASTRUCTURE_VERSION: 1.3.0
# LAST_UPDATED: 2023-12-01

Data Source: MISO
Data Type: load_data
Collection Method: HTTP REST API

import requests
""")

    # Create updater
    updater = ScraperUpdater(scraper_root=str(scraper_root))

    # Step 1: Scan
    scrapers = updater.scan_scrapers()
    assert len(scrapers) == 1
    assert scrapers[0].needs_update is True

    # Step 2: Generate scan report
    scan_report = updater.generate_scan_report(scrapers)
    assert "Outdated Scrapers (1):" in scan_report

    # Step 3: Update scraper
    result = await updater.update_scraper(scrapers[0], validate=False)
    assert result.error is None

    # Step 4: Generate update report
    update_report = updater.generate_update_report([result])
    # Without validation, it should be in "needs review" category
    assert "Updated but Needs Review" in update_report or "Successfully Updated" in update_report

    # Verify scraper was updated
    rescanned = updater.scan_scrapers()
    assert rescanned[0].needs_update is False
    assert rescanned[0].current_version == CURRENT_INFRASTRUCTURE_VERSION
