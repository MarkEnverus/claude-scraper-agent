"""Tests for ScraperFixer."""

import pytest
from pathlib import Path
from unittest.mock import Mock, patch, AsyncMock

from agentic_scraper.fixers.fixer import ScraperFixer, FixerResult


# ============================================================================
# FIXTURES
# ============================================================================

@pytest.fixture
def sample_scraper_content():
    """Sample scraper file content."""
    return """\"\"\"MISO Energy Pricing Scraper - HTTP REST API.

Generated by: Claude Scraper Agent v1.6.0
Infrastructure version: 1.6.0
Generated date: 2024-01-15
Last updated: 2024-01-15

Data Source: MISO
Data Type: energy_pricing
Collection Method: HTTP REST API
\"\"\"

# INFRASTRUCTURE_VERSION: 1.6.0
# LAST_UPDATED: 2024-01-15

import requests

def collect_data():
    base_url = "https://api.misoenergy.org"
    response = requests.get(f"{base_url}/api/v1/data")
    return response.json()
"""


@pytest.fixture
def scraper_file(tmp_path, sample_scraper_content):
    """Create a sample scraper file."""
    scraper_dir = tmp_path / "sourcing" / "scraping"
    scraper_dir.mkdir(parents=True, exist_ok=True)
    scraper_file = scraper_dir / "scraper_miso_energy_pricing_http.py"
    scraper_file.write_text(sample_scraper_content)
    return scraper_file


@pytest.fixture
def fixer(tmp_path):
    """Create ScraperFixer instance with temp directory."""
    scraper_root = tmp_path / "sourcing" / "scraping"
    scraper_root.mkdir(parents=True, exist_ok=True)
    return ScraperFixer(scraper_root=str(scraper_root))


# ============================================================================
# INITIALIZATION TESTS
# ============================================================================

def test_fixer_initialization():
    """Test fixer initializes with correct defaults."""
    fixer = ScraperFixer()
    assert fixer.scraper_root == Path("sourcing/scraping")


def test_fixer_initialization_custom_root():
    """Test fixer initializes with custom root."""
    fixer = ScraperFixer(scraper_root="custom/path")
    assert fixer.scraper_root == Path("custom/path")


# ============================================================================
# SCAN SCRAPERS TESTS
# ============================================================================

def test_scan_scrapers_empty_directory(fixer):
    """Test scanning empty directory returns empty list."""
    scrapers = fixer.scan_scrapers()
    assert scrapers == []


def test_scan_scrapers_finds_scrapers(fixer, scraper_file):
    """Test scanning finds scraper files."""
    scrapers = fixer.scan_scrapers()
    assert len(scrapers) == 1
    assert scrapers[0] == scraper_file


def test_scan_scrapers_multiple_files(tmp_path):
    """Test scanning finds multiple scraper files."""
    scraper_root = tmp_path / "sourcing" / "scraping"
    scraper_root.mkdir(parents=True)

    # Create multiple scrapers
    (scraper_root / "scraper_miso.py").write_text("# scraper 1")
    (scraper_root / "scraper_spp.py").write_text("# scraper 2")
    (scraper_root / "scraper_ercot.py").write_text("# scraper 3")

    fixer = ScraperFixer(scraper_root=str(scraper_root))
    scrapers = fixer.scan_scrapers()

    assert len(scrapers) == 3
    assert all(s.name.startswith("scraper_") for s in scrapers)


def test_scan_scrapers_ignores_non_scraper_files(tmp_path):
    """Test scanning ignores non-scraper files."""
    scraper_root = tmp_path / "sourcing" / "scraping"
    scraper_root.mkdir(parents=True)

    # Create scraper and non-scraper files
    (scraper_root / "scraper_miso.py").write_text("# scraper")
    (scraper_root / "helper.py").write_text("# helper")
    (scraper_root / "test_scraper.py").write_text("# test")

    fixer = ScraperFixer(scraper_root=str(scraper_root))
    scrapers = fixer.scan_scrapers()

    assert len(scrapers) == 1
    assert scrapers[0].name == "scraper_miso.py"


def test_scan_scrapers_nonexistent_root():
    """Test scanning nonexistent directory returns empty list."""
    fixer = ScraperFixer(scraper_root="nonexistent/path")
    scrapers = fixer.scan_scrapers()
    assert scrapers == []


# ============================================================================
# DIAGNOSE ISSUE TESTS
# ============================================================================

def test_diagnose_issue_missing_version(tmp_path):
    """Test diagnosing scraper missing version tracking."""
    scraper_file = tmp_path / "scraper_test.py"
    scraper_file.write_text("# No version info\nimport requests")

    fixer = ScraperFixer()
    diagnosis = fixer.diagnose_issue(scraper_file)

    assert "Missing version tracking" in diagnosis["issues"]
    assert "Add INFRASTRUCTURE_VERSION header" in diagnosis["recommendations"]


def test_diagnose_issue_outdated_imports(tmp_path):
    """Test diagnosing scraper with outdated import patterns."""
    scraper_file = tmp_path / "scraper_test.py"
    scraper_file.write_text("""
# INFRASTRUCTURE_VERSION: 1.6.0
# LAST_UPDATED: 2024-01-15
import requests
""")

    fixer = ScraperFixer()
    diagnosis = fixer.diagnose_issue(scraper_file)

    assert "Outdated import pattern" in diagnosis["issues"]


def test_diagnose_issue_nonexistent_file():
    """Test diagnosing nonexistent file."""
    fixer = ScraperFixer()
    diagnosis = fixer.diagnose_issue(Path("nonexistent.py"))

    assert "File not found" in diagnosis["issues"]


# ============================================================================
# FIX SCRAPER TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_fix_scraper_applies_single_operation(scraper_file):
    """Test fixing scraper with single operation."""
    fixer = ScraperFixer(scraper_root=str(scraper_file.parent))

    fix_operations = [{
        "file": str(scraper_file),
        "old": "https://api.misoenergy.org",
        "new": "https://new-api.misoenergy.org",
    }]

    result = await fixer.fix_scraper(
        scraper_path=scraper_file,
        problem="API endpoint changed",
        fix_operations=fix_operations,
        validate=False,
    )

    # Verify result
    assert isinstance(result, FixerResult)
    assert result.scraper_path == scraper_file
    assert result.problem_description == "API endpoint changed"
    assert len(result.files_modified) == 1

    # Verify file was modified
    content = scraper_file.read_text()
    assert "https://new-api.misoenergy.org" in content
    assert "https://api.misoenergy.org" not in content


@pytest.mark.asyncio
async def test_fix_scraper_applies_multiple_operations(scraper_file):
    """Test fixing scraper with multiple operations."""
    fixer = ScraperFixer(scraper_root=str(scraper_file.parent))

    fix_operations = [
        {
            "file": str(scraper_file),
            "old": "https://api.misoenergy.org",
            "new": "https://new-api.misoenergy.org",
        },
        {
            "file": str(scraper_file),
            "old": "api/v1/data",
            "new": "api/v2/data",
        },
    ]

    result = await fixer.fix_scraper(
        scraper_path=scraper_file,
        problem="API version updated",
        fix_operations=fix_operations,
        validate=False,
    )

    assert len(result.files_modified) == 2

    # Verify all fixes applied
    content = scraper_file.read_text()
    assert "https://new-api.misoenergy.org" in content
    assert "api/v2/data" in content


@pytest.mark.asyncio
async def test_fix_scraper_updates_timestamp(scraper_file):
    """Test fixing scraper updates LAST_UPDATED timestamp."""
    fixer = ScraperFixer(scraper_root=str(scraper_file.parent))

    original_content = scraper_file.read_text()
    original_date = "2024-01-15"
    assert f"# LAST_UPDATED: {original_date}" in original_content

    fix_operations = [{
        "file": str(scraper_file),
        "old": "energy_pricing",
        "new": "energy_data",
    }]

    await fixer.fix_scraper(
        scraper_path=scraper_file,
        problem="Data type updated",
        fix_operations=fix_operations,
        validate=False,
    )

    # Verify timestamp was updated
    content = scraper_file.read_text()
    assert f"# LAST_UPDATED: {original_date}" not in content
    assert "# LAST_UPDATED:" in content


@pytest.mark.asyncio
async def test_fix_scraper_with_validation(scraper_file):
    """Test fixing scraper with validation enabled."""
    fixer = ScraperFixer(scraper_root=str(scraper_file.parent))

    fix_operations = [{
        "file": str(scraper_file),
        "old": "energy_pricing",
        "new": "energy_data",
    }]

    result = await fixer.fix_scraper(
        scraper_path=scraper_file,
        problem="Test fix",
        fix_operations=fix_operations,
        validate=True,  # Enable validation
    )

    # Verify validation was run
    assert result.validation_passed is True


@pytest.mark.asyncio
async def test_fix_scraper_nonexistent_file(fixer):
    """Test fixing nonexistent file raises error."""
    with pytest.raises(FileNotFoundError):
        await fixer.fix_scraper(
            scraper_path=Path("nonexistent.py"),
            problem="Test",
            fix_operations=[],
        )


@pytest.mark.asyncio
async def test_fix_scraper_old_content_not_found(scraper_file):
    """Test fix operation when old content not found."""
    fixer = ScraperFixer(scraper_root=str(scraper_file.parent))

    fix_operations = [{
        "file": str(scraper_file),
        "old": "nonexistent_string",
        "new": "replacement",
    }]

    result = await fixer.fix_scraper(
        scraper_path=scraper_file,
        problem="Test",
        fix_operations=fix_operations,
        validate=False,
    )

    # Operation should complete but with no files modified
    assert len(result.files_modified) == 0


# ============================================================================
# INTEGRATION TESTS
# ============================================================================

@pytest.mark.asyncio
async def test_full_fix_workflow(tmp_path):
    """Test complete fix workflow from scan to fix."""
    # Setup
    scraper_root = tmp_path / "sourcing" / "scraping"
    scraper_root.mkdir(parents=True)

    scraper_file = scraper_root / "scraper_miso.py"
    scraper_file.write_text("""
# INFRASTRUCTURE_VERSION: 1.6.0
# LAST_UPDATED: 2024-01-01

import requests

def collect():
    url = "https://old-api.example.com"
    return requests.get(url)
""")

    # Create fixer
    fixer = ScraperFixer(scraper_root=str(scraper_root))

    # Step 1: Scan
    scrapers = fixer.scan_scrapers()
    assert len(scrapers) == 1

    # Step 2: Diagnose (optional)
    diagnosis = fixer.diagnose_issue(scrapers[0])
    assert isinstance(diagnosis, dict)

    # Step 3: Fix
    result = await fixer.fix_scraper(
        scraper_path=scrapers[0],
        problem="API endpoint changed",
        fix_operations=[{
            "file": str(scrapers[0]),
            "old": "https://old-api.example.com",
            "new": "https://new-api.example.com",
        }],
        validate=False,
    )

    # Verify complete workflow
    assert result.scraper_path == scrapers[0]
    assert len(result.files_modified) > 0

    # Verify fix was applied
    content = scrapers[0].read_text()
    assert "https://new-api.example.com" in content
