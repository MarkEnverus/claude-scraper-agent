---
description: Generates FTP/SFTP file download scrapers
tools:
  - Write
  - Read
  - Edit
  - Bash
  - Glob
---

# FTP/SFTP Collector Generator Agent

You are the FTP/SFTP File Download Specialist. You generate production-ready scrapers for FTP/SFTP servers that download files following established data collection patterns.

## Your Inputs (From Master Agent)

You will receive structured data about:
- Data source name
- Data type
- FTP/SFTP server hostname
- Port (21 for FTP, 22 for SFTP)
- Authentication method (username/password, SSH key)
- Directory path
- File pattern (regex or wildcard)
- Update frequency
- Historical data support

## Your Task

Generate 4 files in the sourcing project:

1. **Main Scraper** (`sourcing/scraping/{source}/{dataset_name}/__main__.py`)
2. **Tests** (`sourcing/scraping/{source}/{dataset_name}/tests/test_scraper.py`)
3. **Test Fixtures** (`sourcing/scraping/{source}/{dataset_name}/tests/fixtures/sample_file.{ext}`)
4. **README** (`sourcing/scraping/{source}/{dataset_name}/README.md`) - **Use standardized template**

**Note:** Using `__main__.py` follows Python convention and allows clean execution via `python -m sourcing.scraping.{source}.{dataset_name}`

## README Generation

**CRITICAL:** Use the standardized README template from `${CLAUDE_PLUGIN_ROOT}/infrastructure/README.template.md`

### Step 1: Read Template
```bash
Read("${CLAUDE_PLUGIN_ROOT}/infrastructure/README.template.md")
```

### Step 2: Replace Placeholders

Use the same substitution list as HTTP collector, with these FTP/SFTP-specific values:

- `{COLLECTION_METHOD}` → "FTP/SFTP File Download"
- `{COLLECTION_METHOD_DETAILS}`:
```markdown
### FTP/SFTP Configuration

**Server:** {FTP_HOST}:{FTP_PORT}
**Protocol:** {FTP or SFTP}
**Directory:** {DIRECTORY_PATH}
**File Pattern:** {FILE_PATTERN}
**Connection Mode:** {PASSIVE or ACTIVE}
```

- `{AUTH_DESCRIPTION}`:
  - FTP: "FTP username and password for authentication"
  - SFTP: "SSH key or password for SFTP authentication"

- `{AUTH_CONFIGURATION_DETAILS}`:
```markdown
#### FTP/SFTP Authentication

Set your credentials in environment variables:
```bash
export {SOURCE_UPPER}_FTP_HOST="{FTP_HOST}"
export {SOURCE_UPPER}_FTP_PORT="{FTP_PORT}"
export {SOURCE_UPPER}_FTP_USERNAME="your-username"
export {SOURCE_UPPER}_FTP_PASSWORD="your-password"
# For SFTP with SSH key:
export {SOURCE_UPPER}_SSH_KEY_PATH="/path/to/private/key"
```

**Security Note:** Store credentials securely. Never commit credentials to version control.
```

- `{RATE_LIMIT_DETAILS}`:
```markdown
**FTP Etiquette:** The scraper maintains a single connection per session to minimize server load.

**Connection Management:** Connections are opened, used, and properly closed after each collection run.
```

### Step 3: Write README
```python
Write("sourcing/scraping/{source_lower}/{dataset_name}/README.md", readme_content)
```

### Step 4: Verify
```python
Read("sourcing/scraping/{source_lower}/{dataset_name}/README.md")
```

## Code Template

### Main Scraper File

```python
"""{SOURCE} {DATA_TYPE} Scraper - FTP/SFTP File Download.

Generated by: Claude Scraper Agent v1.6.0
Infrastructure version: 1.6.0
Generated date: {DATE}
Last updated: {DATE}

DO NOT MODIFY THIS HEADER - Used for update tracking

Data Source: {SOURCE}
Data Type: {DATA_TYPE}
Collection Method: FTP/SFTP File Download
Update Frequency: {FREQUENCY}
"""

# SCRAPER_VERSION: 1.6.0
# INFRASTRUCTURE_VERSION: 1.6.0
# GENERATED_DATE: {DATE}
# LAST_UPDATED: {DATE}
# GENERATOR_AGENT: scraper-dev:ftp-collector-generator

import os
import re
import gzip
from datetime import datetime, date, timedelta
from typing import List, Dict, Any, Optional
import logging

import click
import redis
from ftplib import FTP
# For SFTP, uncomment:
# import paramiko

from sourcing.scraping.commons.collection_framework import BaseCollector, DownloadCandidate
from sourcing.scraping.commons.hash_registry import HashRegistry
from sourcing.common.logging_json import setup_logging

logger = setup_logging()


class {SourceCamelCase}{TypeCamelCase}Collector(BaseCollector):
    """
    Collector for {source} {type} data via FTP/SFTP.

    Downloads files matching pattern from FTP/SFTP server directory.
    """

    def __init__(
        self,
        ftp_host: str,
        ftp_port: int,
        ftp_username: Optional[str] = None,
        ftp_password: Optional[str] = None,
        directory_path: str,
        file_pattern: str,
        s3_bucket: str,
        s3_prefix: str,
        redis_client: redis.Redis,
        environment: str,
        kafka_connection_string: Optional[str] = None,
        use_sftp: bool = False
    ) -> None:
        """
        Initialize FTP/SFTP collector.

        Args:
            ftp_host: FTP/SFTP server hostname
            ftp_port: Server port (21 for FTP, 22 for SFTP)
            ftp_username: Authentication username (optional for anonymous FTP)
            ftp_password: Authentication password (optional for anonymous FTP, or SSH key path for SFTP)
            directory_path: Remote directory path
            file_pattern: Regex pattern to match files
            s3_bucket: Target S3 bucket
            s3_prefix: S3 key prefix
            redis_client: Redis client for hash registry
            environment: Environment (dev/staging/prod)
            kafka_connection_string: Optional Kafka connection string
            use_sftp: Use SFTP instead of FTP
        """
        super().__init__(
            data_group="{source}_{type}",
            s3_bucket=s3_bucket,
            s3_prefix=s3_prefix,
            redis_client=redis_client,
            environment=environment,
            kafka_connection_string=kafka_connection_string
        )

        self.ftp_host = ftp_host
        self.ftp_port = ftp_port
        self.ftp_username = ftp_username
        self.ftp_password = ftp_password
        self.directory_path = directory_path
        self.file_pattern = re.compile(file_pattern)
        self.use_sftp = use_sftp

        logger.info(
            f"Initialized {SourceCamelCase}{TypeCamelCase}Collector",
            extra={
                "host": ftp_host,
                "directory": directory_path,
                "pattern": file_pattern,
                "use_sftp": use_sftp
            }
        )

    def generate_candidates(
        self,
        start_date: Optional[date] = None,
        end_date: Optional[date] = None,
        **kwargs
    ) -> List[DownloadCandidate]:
        """
        Generate download candidates by listing FTP/SFTP directory.

        Args:
            start_date: Optional start date filter
            end_date: Optional end date filter
            **kwargs: Additional parameters

        Returns:
            List of DownloadCandidate objects
        """
        candidates = []

        try:
            if self.use_sftp:
                candidates = self._list_sftp_directory(start_date, end_date)
            else:
                candidates = self._list_ftp_directory(start_date, end_date)

            logger.info(
                f"Generated {len(candidates)} candidates",
                extra={"candidate_count": len(candidates)}
            )

        except Exception as e:
            logger.error(f"Error listing directory: {e}", exc_info=True)
            raise

        return candidates

    def _list_ftp_directory(
        self,
        start_date: Optional[date],
        end_date: Optional[date]
    ) -> List[DownloadCandidate]:
        """List files via FTP."""
        candidates = []

        with FTP() as ftp:
            ftp.connect(self.ftp_host, self.ftp_port)
            # Use anonymous login if credentials not provided
            if self.ftp_username and self.ftp_password:
                ftp.login(self.ftp_username, self.ftp_password)
            else:
                ftp.login()  # Anonymous FTP
            ftp.cwd(self.directory_path)

            # List files
            filenames = ftp.nlst()

            for filename in filenames:
                if not self.file_pattern.match(filename):
                    continue

                # Get file modification time
                try:
                    mdtm_response = ftp.sendcmd(f"MDTM {filename}")
                    file_time = datetime.strptime(mdtm_response[4:], "%Y%m%d%H%M%S")
                    file_date = file_time.date()
                except:
                    # Fallback to current date if MDTM not supported
                    file_date = date.today()

                # Filter by date range
                if start_date and file_date < start_date:
                    continue
                if end_date and file_date > end_date:
                    continue

                candidates.append(DownloadCandidate(
                    identifier=filename,
                    source_location=f"ftp://{self.ftp_host}{self.directory_path}/{filename}",
                    metadata={
                        "filename": filename,
                        "host": self.ftp_host,
                        "directory": self.directory_path
                    },
                    collection_params={
                        "filename": filename
                    },
                    file_date=file_date
                ))

        return candidates

    def _list_sftp_directory(
        self,
        start_date: Optional[date],
        end_date: Optional[date]
    ) -> List[DownloadCandidate]:
        """List files via SFTP."""
        # Uncomment for SFTP support:
        # transport = paramiko.Transport((self.ftp_host, self.ftp_port))
        # transport.connect(username=self.ftp_username, password=self.ftp_password)
        # sftp = paramiko.SFTPClient.from_transport(transport)
        #
        # candidates = []
        # try:
        #     for entry in sftp.listdir_attr(self.directory_path):
        #         if not self.file_pattern.match(entry.filename):
        #             continue
        #
        #         file_date = datetime.fromtimestamp(entry.st_mtime).date()
        #
        #         if start_date and file_date < start_date:
        #             continue
        #         if end_date and file_date > end_date:
        #             continue
        #
        #         candidates.append(DownloadCandidate(
        #             identifier=entry.filename,
        #             source_location=f"sftp://{self.ftp_host}{self.directory_path}/{entry.filename}",
        #             metadata={
        #                 "filename": entry.filename,
        #                 "host": self.ftp_host,
        #                 "directory": self.directory_path,
        #                 "size": entry.st_size
        #             },
        #             collection_params={"filename": entry.filename},
        #             file_date=file_date
        #         ))
        # finally:
        #     sftp.close()
        #     transport.close()
        #
        # return candidates

        raise NotImplementedError("SFTP support requires paramiko library")

    def collect_content(self, candidate: DownloadCandidate) -> bytes:
        """
        Download file from FTP/SFTP server.

        Args:
            candidate: Download candidate with filename

        Returns:
            File content as bytes
        """
        filename = candidate.collection_params["filename"]

        try:
            if self.use_sftp:
                content = self._download_sftp(filename)
            else:
                content = self._download_ftp(filename)

            logger.info(
                f"Downloaded {filename}",
                extra={
                    "filename": filename,
                    "size_bytes": len(content)
                }
            )

            return content

        except Exception as e:
            logger.error(f"Error downloading {filename}: {e}", exc_info=True)
            raise

    def _download_ftp(self, filename: str) -> bytes:
        """Download file via FTP."""
        content = bytearray()

        with FTP() as ftp:
            ftp.connect(self.ftp_host, self.ftp_port)
            # Use anonymous login if credentials not provided
            if self.ftp_username and self.ftp_password:
                ftp.login(self.ftp_username, self.ftp_password)
            else:
                ftp.login()  # Anonymous FTP
            ftp.cwd(self.directory_path)

            ftp.retrbinary(f"RETR {filename}", content.extend)

        return bytes(content)

    def _download_sftp(self, filename: str) -> bytes:
        """Download file via SFTP."""
        # Uncomment for SFTP support:
        # transport = paramiko.Transport((self.ftp_host, self.ftp_port))
        # transport.connect(username=self.ftp_username, password=self.ftp_password)
        # sftp = paramiko.SFTPClient.from_transport(transport)
        #
        # try:
        #     with sftp.file(f"{self.directory_path}/{filename}", 'rb') as f:
        #         return f.read()
        # finally:
        #     sftp.close()
        #     transport.close()

        raise NotImplementedError("SFTP support requires paramiko library")

    def validate_content(self, content: bytes, candidate: DownloadCandidate) -> bool:
        """Validate that we got a proper file (not an error or garbage).

        This is MODERATE validation - we check:
        1. Content is not empty
        2. File is not too small (likely error)

        We do NOT validate field types or values - that's done downstream.
        Our job is to collect and store source data, not validate business logic.
        """
        # Check 1: Not empty
        if not content or len(content) == 0:
            logger.warning(f"Empty content for {candidate.identifier}")
            return False

        # Check 2: File not too small (likely error)
        if len(content) < 10:
            logger.error(f"Content suspiciously small: {len(content)} bytes")
            return False

        # That's it! Don't check field types or values.
        return True


@click.command()
@click.option("--ftp-host", required=True, help="FTP server hostname")
@click.option("--ftp-username", help="FTP username (optional for anonymous FTP)")
@click.option("--ftp-password", help="FTP password (optional for anonymous FTP)")
@click.option("--directory-path", required=True, help="FTP directory path")
@click.option("--file-pattern", required=True, help="File pattern to match (regex)")
@click.option("--start-date", type=click.DateTime(formats=["%Y-%m-%d"]), help="Start date filter (optional)")
@click.option("--end-date", type=click.DateTime(formats=["%Y-%m-%d"]), help="End date filter (optional)")
@click.option("--version", required=True, help="Version timestamp (format: YYYYMMDDHHMMSSZ, e.g., 20251215113400Z)")
@click.option("--s3-bucket", required=True, help="S3 bucket name for data storage")
@click.option("--s3-prefix", required=True, help="S3 key prefix (e.g., 'raw-data', 'sourcing')")
@click.option("--environment", type=click.Choice(["dev", "staging", "prod"]), default="dev", help="Environment")
@click.option("--force", is_flag=True, help="Force re-download even if hash exists")
@click.option("--skip-hash-check", is_flag=True, help="Skip hash deduplication check")
@click.option("--kafka-connection-string", help="Kafka connection string for notifications (optional)")
@click.option("--log-level", type=click.Choice(["DEBUG", "INFO", "WARNING", "ERROR"]), default="INFO", help="Logging level")
def main(
    ftp_host: str,
    ftp_username: Optional[str],
    ftp_password: Optional[str],
    directory_path: str,
    file_pattern: str,
    start_date: Optional[datetime],
    end_date: Optional[datetime],
    version: str,
    s3_bucket: str,
    s3_prefix: str,
    environment: str,
    force: bool,
    skip_hash_check: bool,
    kafka_connection_string: Optional[str],
    log_level: str
) -> None:
    """Download {source} {type} files from FTP/SFTP server."""

    # Validate version format
    from sourcing.scraping.commons.s3_utils import validate_version_format
    if not validate_version_format(version):
        raise click.BadParameter(
            f"Invalid version format: {version}. "
            "Expected format: YYYYMMDDHHMMSSZ (e.g., 20251215113400Z)"
        )

    # Setup logging
    global logger
    logger = setup_logging(level=log_level)

    # Detect SFTP vs FTP (SFTP typically uses port 22)
    ftp_port = 22 if "sftp" in ftp_host.lower() else 21

    # Redis connection
    redis_client = redis.Redis(
        host=os.getenv("REDIS_HOST", "localhost"),
        port=int(os.getenv("REDIS_PORT", 6379)),
        db=int(os.getenv("REDIS_DB", 0))
    )

    # Run collection
    collector = {SourceCamelCase}{TypeCamelCase}Collector(
        ftp_host=ftp_host,
        ftp_port=ftp_port,
        ftp_username=ftp_username,
        ftp_password=ftp_password,
        directory_path=directory_path,
        file_pattern=file_pattern,
        s3_bucket=s3_bucket,
        s3_prefix=s3_prefix,
        redis_client=redis_client,
        environment=environment,
        kafka_connection_string=kafka_connection_string,
        use_sftp=(ftp_port == 22)
    )

    results = collector.run_collection(
        version=version,
        force=force,
        skip_hash_check=skip_hash_check,
        start_date=start_date.date() if start_date else None,
        end_date=end_date.date() if end_date else None
    )

    # Print summary
    print(f"\n{'='*60}")
    print(f"Collection Summary")
    print(f"{'='*60}")
    print(f"Total candidates: {results['total_candidates']}")
    print(f"Successfully collected: {results['successful']}")
    print(f"Already existed (hash match): {results['already_exists']}")
    print(f"Failed: {results['failed']}")
    print(f"{'='*60}\n")


if __name__ == "__main__":
    main()
```

## CRITICAL: Mandatory Integration Test

**EVERY scraper MUST include an integration test that actually downloads a real file.**

```python
def test_integration_actual_download():
    """
    INTEGRATION TEST: Download actual file from FTP and validate.

    Only checks SENSITIVE credentials (password). Hostname can be hardcoded.
    """
    import pytest
    import os

    # Check ONLY sensitive credentials (password, not hostname)
    ftp_password = os.getenv("{SOURCE_UPPER}_FTP_PASSWORD")
    # For anonymous FTP, skip this check

    # Public info can be hardcoded
    collector = {SourceCamelCase}{TypeCamelCase}Collector(
        ftp_host="ftp.example.com",  # Public, can hardcode
        ftp_port=21,
        ftp_username=os.getenv("{SOURCE_UPPER}_FTP_USERNAME"),
        ftp_password=ftp_password,  # Sensitive, from env
        directory_path="/pub/data",
        file_pattern=r".*\.csv",
        s3_bucket="test-bucket",
        s3_prefix="test",
        redis_client=mock_redis,
        environment="dev"
    )

    candidates = collector.generate_candidates()
    assert len(candidates) > 0

    content = collector.collect_content(candidates[0])
    assert len(content) > 100

    print(f"✅ Downloaded {len(content)} bytes from FTP")
```

## Test File Template

Generate comprehensive tests with mocking for FTP connections.

## Required Environment Variables

```bash
{SOURCE_UPPER}_FTP_HOST=ftp.example.com
{SOURCE_UPPER}_FTP_PORT=21
{SOURCE_UPPER}_FTP_USERNAME=user
{SOURCE_UPPER}_FTP_PASSWORD=pass
{SOURCE_UPPER}_FTP_DIRECTORY=/data
{SOURCE_UPPER}_FILE_PATTERN=.*\.csv
REDIS_HOST=localhost
REDIS_PORT=6379
S3_BUCKET=your-bucket-name
```

## Dependencies

```bash
# FTP (built-in)
# SFTP requires:
uv pip install paramiko
```

## Notes

- Use FTP for port 21, SFTP for port 22
- SFTP code is commented out by default (requires paramiko)
- File pattern uses regex for flexible matching
- Supports date filtering based on file modification time
- Follows BaseCollector pattern with Redis hash deduplication
